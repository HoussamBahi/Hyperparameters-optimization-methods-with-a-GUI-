## LIBRARIES IMPORT


#from tensorflow.python.keras.optimizers import RMSprop


#TF_UNOFFICIAL_SETTING=1 ./configure
#bazel build -c opt --config=cuda
## Accessing google servers
from numbers import Real

import tensorflow as tf
from dask.utils import K
from keras.constraints import maxnorm

from keras.losses import MSE
from keras.optimizers import RMSprop
from pandas.plotting._matplotlib import hist
import keras
from importlib import reload


from skopt import gp_minimize
from skopt.space import Integer, Categorical, Real
from skopt.utils import use_named_args

reload(keras.models)
#import h5py

print("\n----------------------------------------")
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
tf.config.experimental
# for device in tf.config.experimental.list_physical_devices("GPU"):
 #  tf.config.experimental.set_memory_growth(device, True)
print("Importing libraries...")
gpus = tf.config.experimental.list_physical_devices(device_type='GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

import os
import matplotlib.pyplot as plt

import time
import numpy as np
import random
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.callbacks import EarlyStopping

## SEED FIX
print("\n----------------------------------------")
print("Fixing seed...")
##random_state = 42

random_state = 42
seed = np.random.seed(random_state)
tf.compat.v1.set_random_seed(random_state)

# seed = np.random.seed(random_state)
##tf.random.set_seed(random_state)

print("  -> Some random number to check if seed fix works: %f (numpy) ; %f (tf)"%(np.random.random(), tf.random.uniform((1,1))[0][0]))

#physical_devices = tf.config.list_physical_devices('GPU') tf.config.experimental.set_memory_growth(physical_devices[0], True)

## SAVE PATH
NAME = "Bayesian_search_2"
#NAME = "test10_ann_50_50_b1_mpe"# Name of the current test

#SAVE_PATH = "C:/Users//houssem//Desktop//final project//virial_prediction_v10//result//" + NAME + "//"  # path where results are saved
SAVE_PATH = "E://" + NAME + "//"
os.makedirs(SAVE_PATH, exist_ok=True)
os.makedirs(SAVE_PATH + "//model_intermediate//", exist_ok=True)
os.makedirs(SAVE_PATH + "//model_final//", exist_ok=True)

## LOADING DATA
print("\n----------------------------------------")
print("Loading data...")
# C:\Users\houssem\Desktop\final project
# Paths and format
PATH = "C:/Users//houssem//Desktop//final project//virial_prediction_v10"
VirialFullPath = PATH + "//dataset_virial//allVirial-Mix.csv"
CriticalPath = PATH + "//dataset_phy//CriticalPropreties_v2.csv"
DipolePath = PATH + "//dataset_phy//DipoleMomentYaws_v0.csv"
delimiter = ";"

# Loading data
CriticalCsv = np.loadtxt(CriticalPath, dtype='str', delimiter=delimiter)
CriticalLeg = CriticalCsv[0, :]  # extracting legends
CriticalNom = CriticalCsv[1:, 0:3]  # extracting nomenclature CriticalNom = [Name,Formula,#CAS]
CriticalData = CriticalCsv[1:, 3:9].astype(
    float)  # extracting data CriticalData = [Molwt, TcK, PcMPa, Vcm3/kmol, Zc, AcentricFactor]
CriticalRef = CriticalCsv[1:, 9]  # extracting references

DipoleCsv = np.loadtxt(DipolePath, dtype='str', delimiter=delimiter)
DipoleLeg = DipoleCsv[0, :]  # extracting legends
DipoleNom = DipoleCsv[1:, 0:4]  # extracting nomenclature & state DipoleNom = [Name,Formula,#CAS,state]
DipoleData = DipoleCsv[1:, 4].astype(float)  # extracting data DipoleData = [DipoleMomentDebye]
DipoleRef = DipoleCsv[1:, 5]  # extracting references

VirialPath = VirialFullPath
VirialCsv = np.loadtxt(VirialPath, dtype='str', delimiter=delimiter)
VirialLeg = VirialCsv[0, :]  # extracting legends
VirialNom = VirialCsv[1:, 0:4]  # VirialNom = [Formula1,CASno1,Formula2,CASno2]
VirialRef = VirialCsv[1:, 7]  # VirialRef = [ref]

VirialUncertainties = VirialCsv[1:, 6].astype(float)  # VirialUncertainties = [Uncertainties]
VirialData = VirialCsv[1:, 4:6].astype(float)  # VirialData = [T (K),B12 (cm3/mol)]

# todo: 1 use only Tr, Tc, Pc, and Ï‰ (Dinicola)
## NEURAL NETWORK INPUT/OUTPUT
print("\n----------------------------------------")
print("Creating neural network input/output...")


def get_CriticalPropreties(CAS, CriticalNom, CriticalData):
    try:
        ind = np.where(CriticalNom[:, 2] == CAS)[0][0]
        return CriticalData[ind]
    except:
        return np.zeros((6), dtype=bool)


def get_DipoleMoment(CAS, DipoleNom, DipoleData):
    try:
        ind = np.where(DipoleNom[:, 2] == CAS)[0][0]
        return DipoleData[ind]
    except:
        return ('False')


X = []
Y = []
molecules_with_unfound_Critical_propreties = []
molecules_with_unfound_Dipole_Moment = []
mix_with_big_uncert = []
nb_of_unusable_data = 0

for i in range(len(VirialData)):
    CAS1 = VirialNom[i, 1]
    CAS2 = VirialNom[i, 3]
    if random.random() > 0.5:
        CAS1, CAS2 = CAS2, CAS1  # random shuffle
    Critical1 = get_CriticalPropreties(CAS1, CriticalNom, CriticalData)
    Critical2 = get_CriticalPropreties(CAS2, CriticalNom, CriticalData)
    Dipole1 = get_DipoleMoment(CAS1, DipoleNom, DipoleData)
    Dipole2 = get_DipoleMoment(CAS2, DipoleNom, DipoleData)
    if (Critical1[0] != False and Critical2[0] != False and Dipole1 != 'False' and Dipole2 != 'False' and abs(
            VirialUncertainties[i]) < 50):
        # X.append(np.concatenate((Critical1,[Dipole1],Critical2,[Dipole2],[VirialData[i,0]],[VirialUncertainties[i]])))
        # X.append(np.concatenate((Critical1,[Dipole1],Critical2,[Dipole2],[VirialData[i,0]])))
        # X.append(np.concatenate((Critical1[1:],[Dipole1],Critical2[1:],[Dipole2],[VirialData[i,0]])))
        # X.append(np.concatenate((Critical1[1:4],[Critical1[5]],[Dipole1],Critical2[1:4],[Critical2[5]],[Dipole2],[VirialData[i,0]])))
        X.append(np.concatenate((Critical1[1:4], [Critical1[5]], [Dipole1], Critical2[1:4], [Critical2[5]], [Dipole2],
                                 [VirialData[i, 0] / Critical1[1]])))
        Y.append(VirialData[i, 1])
    else:
        nb_of_unusable_data += 1
        if (Critical1[0] == False):
            molecules_with_unfound_Critical_propreties.append(CAS1)
        if (Critical2[0] == False):
            molecules_with_unfound_Critical_propreties.append(CAS2)
        if (Dipole1 == 'False'):
            molecules_with_unfound_Dipole_Moment.append(CAS1)
        if (Dipole2 == 'False'):
            molecules_with_unfound_Dipole_Moment.append(CAS2)
        if (abs(VirialUncertainties[i]) > 50):
            mix_with_big_uncert.append((CAS1, CAS2))

X = np.array(X)
Y = np.array(Y)
N, inpSize = X.shape
print("Unusable data: %.2f %s" % (100 * nb_of_unusable_data / len(VirialData), '%'))

## NORMALIZING DATA
print("\n----------------------------------------")
print("Normalizing data...")
# todo: 2 use normalization

# scalerX = prepro.MinMaxScaler()
# scalerX.fit(X)
# normalizedX = scalerX.transform(X)
# X = normalizedX
#
# Y = Y.reshape(-1, 1)
# scalerY = prepro.MinMaxScaler()
# scalerY.fit(Y)
# normalizedY = scalerY.transform(Y)
# Y = normalizedY

# inverse transform
# inverse = scaler.inverse_transform(normalizedX)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42, shuffle=True)

## NEURAL NETWORK ARCHITECTURE
print("\n----------------------------------------")
print("Neural network architecture...")
from tkinter import *
def iteration():
    v = e9.get()

    num = int(v)
    print(num)
    ite = num
    lab91 = Label(root, text=ite).grid(row=8, column=3)
    e9.insert(0, "")
    return ite

def layers():
    v = e1.get()

    num = int(v)
    print(num)
    layer.append(num)
    lab11 = Label(root, text=layer).grid(row=0, column=3)
    e1.insert(0,'')
    return layer

def neurons():
    v = e6.get()

    num = int(v)
    print(num)
    neuron.append(num)
    lab71 = Label(root, text=neuron).grid(row=5, column=3)
    e1.insert(0,'')
    return neuron
def Batch():
    v = e4.get()

    num = int(v)
    print(num)
    batch=num
    lab41 = Label(root, text=batch).grid(row=3, column=3)
    e4.insert(0,'')
    return batch
def opti():

    opt = e2.get()
    print(opt)
    optimi.append(opt)
    print(optimi)
    lab21 = Label(root, text=optimi).grid(row=1, column=3)
    e2.insert(0,"")
    return optimi
def acti():

    act = e8.get()
    print(act)
    activi.append(act)
    print(activi)
    lab81 = Label(root, text=activi).grid(row=7, column=3)
    e8.insert(0,"")
    return activi
def init_mod():

    initia = e5.get()
    print(initia)
    ini.append(initia)
    print(ini)
    lab21 = Label(root, text=ini).grid(row=4, column=3)
    e5.insert(0,"")
    return ini
def epoch():

    num = e3.get()
    print(num)
    epo = int(num)

    lab31 = Label(root, text=epo).grid(row=2, column=3)

    return epo

def splits():

    num = e7.get()
    print(num)
    split = int(num)

    lab61 = Label(root, text=split).grid(row=6, column=3)

    return split
def bay():
    bayes()


# the first gui owns the root window
if __name__ == "__main__":
    root = Tk()
    root.title('Random Search')
    root.minsize(920, 800)
    scrollbar = Scrollbar(root, orient="vertical")
    scrollbar.grid(row=9, column=1)

    mylist = Listbox(root, width=140, yscrollcommand=scrollbar.set)
    mylist.grid(row=9, column=1, rowspan=4,
                columnspan=6)
    scrollbar.config(command=mylist.yview)

    button_1 = Button(root, text='Start Random Search', width='20', height='5', command=bay)
    button_1.grid(row=13, column=4)
    lab1= Label(root, text="nb of layers ").grid(row=0)
    lab2= Label(root, text="optimizer").grid(row=1)
    lab3= Label(root, text="Epoch").grid(row=2)
    lab4= Label(root, text="Batch size").grid(row=3)
    lab5 = Label(root, text="initializer").grid(row=4)
    lab6 = Label(root, text="neurons").grid(row=5)
    lab7 = Label(root, text="split number (cv)").grid(row=6)
    lab8 = Label(root, text="activation function").grid(row=7)
    lab9 = Label(root, text="Iteration number").grid(row=8)
    e9=Entry(root)
    e9.grid(row=8,column=1)
    e8=Entry(root)
    e8.grid(row=7,column=1)
    e7= Entry(root)
    e7.grid(row=6, column=1)
    e6 = Entry(root)
    e6.grid(row=5, column=1)
    e5= Entry(root)
    e5.grid(row=4, column=1)
    e4= Entry(root)
    e4.grid(row=3, column=1)
    e3= Entry(root)
    e3.grid(row=2, column=1)
    e1 = Entry(root)
    e1.insert(0, "2")
    e2 = Entry(root)
    e2.insert(0, "SGD")
    e1.grid(row=0, column=1)
    e2.grid(row=1, column=1)
    e7.insert(0,"5")
    button_3 = Button(root, text='add optimizer ', width='20', height='2', command=opti)
    button_3.grid(row=1, column=2)
    #init=init_mode

    button_2 = Button(root, text='add layer ', width='20', height='2', command=layers)
    button_2.grid(row=0, column=2)
    if e1.get() == "0":
        e1.insert(0,"2")
        print(e1.get())
    if e2.get()=="0":
        e2.insert(0,"SGD")

    button_4=Button(root, text='add Epoch number ', width='20', height='2', command=epoch)
    button_4.grid(row=2, column=2)

    button_5 = Button(root, text='add Batch size ', width='20', height='2', command=Batch)
    button_5.grid(row=3, column=2)

    button_6 = Button(root, text='add initializer ', width='20', height='2', command=init_mod)
    button_6.grid(row=4, column=2)

    button_7 = Button(root, text='add neurons ', width='20', height='2', command=neurons)
    button_7.grid(row=5, column=2)

    button_8 = Button(root, text='add split number ', width='20', height='2', command=splits)
    button_8.grid(row=6, column=2)

    button_9 = Button(root, text='add activation ', width='20', height='2', command=acti)
    button_9.grid(row=7, column=2)

    button_10 = Button(root, text='add iteration number ', width='20', height='2', command=iteration)
    button_10.grid(row=8, column=2)

    layer=[2]
    optimi=['SGD']
    epo=1
    batch=[1]
    ini=['uniform']
    neuron=[50]
    split=5
    activi=['softsign']
    ite=11

    lab11= Label(root,text=layer).grid(row=0,column=3)
    lab21= Label(root,text=optimi).grid(row=1,column=3)
    lab31 = Label(root, text=epo).grid(row=2, column=3)
    lab41 = Label(root, text=batch).grid(row=3, column=3)
    lab51 = Label(root, text=ini).grid(row=4, column=3)
    lab61 = Label(root, text=split).grid(row=6, column=3)
    lab71 = Label(root, text=neuron).grid(row=5, column=3)
    lab81 = Label(root, text=activi).grid(row=7, column=3)


    root.mainloop()
l1=layer[0]
l2=layer[1]

dim_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',
                         name='learning_rate')
dim_num_dense_layers = Integer(low=l1, high=l2, name='num_dense_layers')
dim_num_dense_nodes = Integer(low=50, high=150, name='num_dense_nodes')
dim_activation = Categorical(categories=['relu', 'sigmoid','softsign','tanh'],
                             name='activation')
dim_initializer = Categorical(categories=['uniform', 'normal'],
                             name='kernel_initializer')
dimensions = [dim_learning_rate,
              dim_num_dense_layers,
              dim_num_dense_nodes,
              dim_activation,
              dim_initializer]
default_parameters = [1e-4, 3, 150, 'tanh','normal']

# todo: 4 struct optimization (2 x 19 in Dinicola)
def architecture(learning_rate,num_dense_layers, num_dense_nodes,activation,kernel_initializer):
    initializer = "uniform"
    model = tf.keras.Sequential()
    #model.add(tf.keras.layers.Dropout(0.0))
    for i in range(num_dense_layers):
        model.add(
            tf.keras.layers.Dense(num_dense_nodes, input_dim=11, kernel_initializer=kernel_initializer, activation=activation))
    model.add(tf.keras.layers.Dense(1, kernel_initializer=kernel_initializer))
    OPTIMIZER = tf.optimizers.RMSprop(learning_rate=learning_rate, rho=0.9, epsilon=None,
                                      decay=0.0)  # todo: optimize optimizer
    LOSS = tf.keras.losses.MSE
    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=[tf.keras.losses.MSE, "mean_absolute_percentage_error"])

    return model


@use_named_args(dimensions=dimensions)
def fitness(learning_rate, num_dense_layers,
            num_dense_nodes, activation,kernel_initializer):
    """
    Hyper-parameters:
    learning_rate:     Learning-rate for the optimizer.
    num_dense_layers:  Number of dense layers.
    num_dense_nodes:   Number of nodes in each dense layer.
    activation:        Activation function for all layers.
    """

    # Print the hyper-parameters.
    print('learning rate: {0:.1e}'.format(learning_rate))
    print('num_dense_layers:', num_dense_layers)
    print('num_dense_nodes:', num_dense_nodes)
    print('activation:', activation)
    print('kernel_initializer: ', kernel_initializer)
    print()

    # Create the neural network with these hyper-parameters.
    model = architecture(learning_rate=learning_rate,
                         num_dense_layers=num_dense_layers,
                         num_dense_nodes=num_dense_nodes,
                         activation=activation,
                         kernel_initializer=kernel_initializer)

    # Dir-name for the TensorBoard log-files.
    # log_dir = log_dir_name(learning_rate, num_dense_layers,
    #                        num_dense_nodes, activation)

    # Create a callback-function for Keras which will be
    # run after each epoch has ended during training.
    # This saves the log-files for TensorBoard.
    # Note that there are complications when histogram_freq=1.
    # It might give strange errors and it also does not properly
    # support Keras data-generators for the validation-set.
    # callback_log = TensorBoard(
    #     log_dir=log_dir,
    #     histogram_freq=0,
    #     write_graph=True,
    #     write_grads=False,
    #     write_images=False)

    # Use Keras to train the model.
    history = model.fit(x=X_train,
                        y=Y_train,
                        epochs=1,
                        batch_size=16,
                        validation_data=(X_test, Y_test)
                       )

    # Get the classification accuracy on the validation-set
    # after the last training-epoch.
    accuracy = history.history['loss'][-1]

    # Print the classification accuracy.
    print()
    print("Accuracy: {0:.2%}".format(accuracy))
    print()

    # Save the model if it improves on the best-found performance.
    # We use the global keyword so we update the variable outside
    # of this function.
    global best_accuracy

    # If the classification accuracy of the saved model is improved ...
    # if accuracy > best_accuracy:
    #     # Save the new model to harddisk.
    #     ##model.save(path_best_model)
    #
    #     # Update the classification accuracy.
    #     best_accuracy = accuracy
    #
    # # Delete the Keras model with these hyper-parameters from memory.
    # del model
    #
    # # Clear the Keras session, otherwise it will keep adding new
    # # models to the same TensorFlow graph each time we create
    # # a model with a different set of hyper-parameters.
    # K.clear_session()

    # NOTE: Scikit-optimize does minimization so it tries to
    # find a set of hyper-parameters with the LOWEST fitness-value.
    # Because we are interested in the HIGHEST classification
    # accuracy, we need to negate this number so it can be minimized.
    return accuracy

fitness(x= default_parameters)
def bayes():

 search_result = gp_minimize(func=fitness,
                            dimensions=dimensions,
                            acq_func='EI', # Expected Improvement.
                            n_calls=11,
                            x0=default_parameters)
 best=search_result.x

 print(best)
 best_results= sorted(zip(search_result.func_vals, search_result.x_iters))
 print(best_results)

 from skopt.plots import plot_objective_2D, plot_objective

 fig = plot_objective_2D(result=search_result,
                        dimension_identifier1='learning_rate',
                        dimension_identifier2='num_dense_nodes',
                        levels=50)
 plt.savefig(SAVE_PATH + "Lr_numnods.png", dpi=400)
 plt.show()

# create a list for plotting
 dim_names = ['learning_rate', 'num_dense_layers', 'num_dense_nodes', 'activation','kernel_initializer' ]
 fig, ax ,*is_anything_else_being_returned= plot_objective(result=search_result, dimensions=dim_names)
 plt.savefig(SAVE_PATH+"all_dimen.png", dpi=400)
 plt.show()
# This function exactly comes from :Hvass-Labs, TensorFlow-Tutorials
## HYPERPARAMETERS
print("\n----------------------------------------")
print("Hyperparameters...")

# BATCH SIZE
BATCH_SIZE = 1
# todo: optimize batch size ?

# OPTIMIZER
# keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-6)
#OPTIMIZER = tf.optimizers.RMSprop(learning_rate=0.0005, rho=0.9, epsilon=None, decay=0.0)  # todo: optimize optimizer
# OPTIMIZER = tf.optimizers.RMSprop(lr=0.1, rho=0.9, epsilon=None, decay=0.0)
#OPTIMIZER = tf.optimizers.Adam(lr=1e-3)
# OPTIMIZER = tf.optimizers.SGD(lr=0.01, clipnorm=1.)
# OPTIMIZER = tf.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)

# LOSS
LOSS = tf.keras.losses.MSE
#LOSS = "mean_absolute_percentage_error"

# todo: 3 optimize loss

# EPOCHS
EPOCHS = 10000

# MODEL
#model = architecture(inpSize)
#model.compile(loss=LOSS, optimizer= OPTIMIZER, metrics=[ tf.keras.losses.MSE,"mean_absolute_percentage_error"])
#model.summary()
# pretrained weights:
#tf.keras.models.load_model("C:/Users//houssem//Desktop//final project//virial_prediction_v10//results//test10_ann_50_50_b1_mpe//model_intermediate//test10_ann_50_50_b1_mpe_1501.h5")
#model.load_weights("C:/Users//houssem//Desktop//final project//virial_prediction_v10//result//testnbneurons142//model_intermediate//testnbneurons142_15000.h5")
#model.load_weights("E:/adam_relu_long_shot//model_intermediate//adam_relu_long_shot_1701.h5")
# todo: try train using MSE after MPE

## CALLBACKS
# model_checkpoint = ModelCheckpoint(SAVE_PATH + "//model_intermediate//" + NAME + '_{epoch:1d}' + '.h5',
#                                    monitor='val_loss',
#                                    verbose=1,
#                                    save_best_only=False,
#                                    save_weights_only=True)
# #earlystopper = EarlyStopping(monitor='val_loss', mode='min', restore_best_weights=True, verbose=1)
#
# tensorboard = TensorBoard(SAVE_PATH + '//logs//',
#                                profile_batch=0)

## TRAINING
print("\n----------------------------------------")
print("Training...")

t1 = time.time()
# Fit the model on the dataset
# history = model.fit(x=X_train, y=Y_train,
#                     batch_size=BATCH_SIZE,
#                     epochs=EPOCHS,
#                     validation_data=(X_test, Y_test),
#                     verbose=2,
#                     callbacks=[model_checkpoint,tensorboard])

t2 = time.time()
computing_time = t2 - t1
print('\n')
print('Training Time =', computing_time, 's')

#python3 -m tensorboard.main --logdir=logs --port=6006

## SAVING FINAL MODEL
print("\n----------------------------------------")
print("Saving final model...")
#
# model.save_weights(SAVE_PATH + "//model_final//" + NAME + '_final_weights.h5')
# model.save(SAVE_PATH + "//model_final//" + NAME + '_my_model.h5')  # creates a HDF5 file 'my_model.h5'
# model_json = model.to_json()
#
# with open(SAVE_PATH + "//model_final//" + NAME + "_model.json", "w") as json_file:
#     json_file.write(model_json)
#
# print("Saved model to disk")

## MODEL PERFORMANCES
print("\n----------------------------------------")
print("Assessing final model performances...")

#preds = model.predict(X_test)[:, 0]

# Inverse normalization
# Y_test =  scalerY.inverse_transform(Y_test)
# preds = preds.reshape(-1, 1)
# preds = scalerY.inverse_transform(preds)

# ME = np.mean(abs(preds - Y_test))
# RMSE = np.sqrt(np.mean((preds - Y_test) ** 2))
# MPE = np.mean(100 * (abs((preds - Y_test) / Y_test)) ** 1)
# print("ME = %.2f cm3/mol" % (ME))
# print("RMSE = %.2f cm3/mol" % (RMSE))
# print("MPE = %.2f percts" % (MPE))

t2 = time.time()
print("Comput. time = %.2f s" % (t2 - t1))

## BEST MODEL PERFS


## TRAINING GRAPHS

# epochs_loss_train = np.array(history.history['loss'])
# epochs_loss_val = np.array(history.history['val_loss'])
# plt.figure()
# plt.title("Loss")
# plt.plot(epochs_loss_train, 'b-',
#          label="training (best %.2f %% at epoch %i)" % (epochs_loss_train.min(), epochs_loss_train.argmin()))
# plt.plot(epochs_loss_val, 'r-',
#          label="validation (best %.2f %% at epoch %i)" % (epochs_loss_val.min(), epochs_loss_val.argmin()))
# plt.legend()
# plt.savefig(SAVE_PATH + "epochs_loss.png")
# plt.show()
#
# # RMSE graph
# epochs_RMSE_train = np.sqrt(history.history['mean_squared_error'])
# epochs_RMSE_val = np.sqrt(history.history['val_mean_squared_error'])
# plt.figure()
# plt.title("RMSE")
# plt.plot(epochs_RMSE_train, 'b-',
#          label="training (best %.2f cm3/mol at epoch %i)" % (epochs_RMSE_train.min(), epochs_RMSE_train.argmin()))
# plt.plot(epochs_RMSE_val, 'r-',
#          label="validation (best %.2f cm3/mol at epoch %i)" % (epochs_RMSE_val.min(), epochs_RMSE_val.argmin()))
# plt.legend()
# plt.savefig(SAVE_PATH + "epochs_RMSE.png")
# plt.show()

# MPE graph
#epochs_MPE_train = np.sqrt(history.history['mean_absolute_percentage_error'])
#epochs_MPE_val = np.sqrt(history.history['val_mean_absolute_percentage_error'])
#plt.figure()
#plt.title("MPE")
#plt.plot(epochs_MPE_train, 'b-',
#         label="training (best %.2f %% at epoch %i)" % (epochs_MPE_train.min(), epochs_MPE_train.argmin()))
#plt.plot(epochs_MPE_val, 'r-',
#         label="validation (best %.2f %% at epoch %i)" % (epochs_MPE_val.min(), epochs_MPE_val.argmin()))
#plt.legend()
#plt.savefig(SAVE_PATH + "epochs_MPE.png")
#plt.show()


## Predict B12 of some molecule pairs
#
# def pair_prediction(model, CAS1, CAS2, CriticalNom, CriticalData, DipoleNom, DipoleData, save=False):
#     crit1 = get_CriticalPropreties(CAS1, CriticalNom, CriticalData)
#     crit2 = get_CriticalPropreties(CAS2, CriticalNom, CriticalData)
#     dip1 = get_DipoleMoment(CAS1, DipoleNom, DipoleData)
#     dip2 = get_DipoleMoment(CAS1, DipoleNom, DipoleData)
#     temps = np.arange(100, 600, 2)
#     Xpp = []
#     for T in temps:
#         # xpp = np.concatenate((crit1,[dip1],crit2,[dip2],[T],[1]))
#         xpp = np.concatenate((crit1[1:4], [crit1[5]], [dip1], crit2[1:4], [crit2[5]], [dip2], [T / crit1[1]]))
#         # xpp = np.concatenate((crit1,[dip1],crit2,[dip2],[T]))
#         Xpp.append(xpp)
#     Xpp = np.array(Xpp)
#     # normalizedXpp = scalerX.transform(Xpp)
#     # normalizedYpp = model.predict(normalizedXpp)[:,0]
#     # normalizedYpp = normalizedYpp.reshape(-1, 1)
#     # Ypp = scalerY.inverse_transform(normalizedYpp)
#     Ypp = model.predict(Xpp)[:, 0]
#     fig = plt.figure(figsize=(9, 5))
#     plt.title("pred of %s + %s" % (CAS1, CAS2))
#     # ploting exp data
#     for i in range(len(VirialNom)):
#         if ((CAS1 == VirialNom[i, 1] and CAS2 == VirialNom[i, 3]) or (
#                 CAS2 == VirialNom[i, 1] and CAS1 == VirialNom[i, 3])):
#             plt.plot(VirialData[i, 0], VirialData[i, 1], 'r+', label="exp: " + VirialRef[i])
#     plt.legend(bbox_to_anchor=(1.05, 1), loc=0, borderaxespad=0.)
#     plt.plot(temps, Ypp, 'k-.', label="B12 prediction for " + CAS1 + " & " + CAS2)
#     if save is not False:
#         plt.savefig(save)
#     plt.show()


#best_model_loss = architecture(inpSize)
#best_model_loss.load_weights(SAVE_PATH + "//model_intermediate//" + NAME + "_" + str(epochs_loss_val.argmin()) + ".h5")

# Ne:7440-01-9, Xe:7440-63-3
#pair_prediction(best_model_loss, "7440-01-9", "7440-63-3", CriticalNom, CriticalData, DipoleNom, DipoleData,
#save=SAVE_PATH + "Ne_Xe_pred_bestloss.png")
# CO2:124-38-9, O2:7782-44-7
#pair_prediction(best_model_loss, "124-38-9", "7782-44-7", CriticalNom, CriticalData, DipoleNom, DipoleData,
#save=SAVE_PATH + "CO2_O2_pred_bestloss.png")

# best_model_RMSE = architecture(inpSize)
# best_model_RMSE.load_weights(SAVE_PATH + "//model_intermediate//" + NAME + "_" + str(epochs_RMSE_val.argmin()) + ".h5")
#
# # Ne:7440-01-9, Xe:7440-63-3
# pair_prediction(best_model_RMSE, "7440-01-9", "7440-63-3", CriticalNom, CriticalData, DipoleNom, DipoleData,
# save=SAVE_PATH + "Ne_Xe_pred_bestRMSE.png")
# # CO2:124-38-9, O2:7782-44-7
# pair_prediction(best_model_RMSE, "124-38-9", "7782-44-7", CriticalNom, CriticalData, DipoleNom, DipoleData,
# save=SAVE_PATH + "CO2_O2_pred_bestRMSE.png")
